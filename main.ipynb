{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch import optim\n",
    "from collections import deque\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import Conv_Att_MCI_Dataset\n",
    "from models import BaseModel, VGG16GradCAM, ConvAttnModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "img_type = 'all'\n",
    "\n",
    "# Model\n",
    "backbone_freezing = True\n",
    "## Conv-Att\n",
    "h_dim_attn = 128\n",
    "n_heads = 1\n",
    "h_dim_fc = 512\n",
    "n_layers = 1\n",
    "\n",
    "# training\n",
    "batch_size = 32#64\n",
    "n_epochs = 100\n",
    "best_loss = np.inf\n",
    "best_epoch = 0\n",
    "best_flag = False\n",
    "\n",
    "# optimizer\n",
    "lr = 1e-5\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.99\n",
    "eps = 1e-7\n",
    "\n",
    "## Early stopping\n",
    "es_size = 4\n",
    "\n",
    "# Save directory\n",
    "savedir = './checkpoint'\n",
    "os.makedirs(savedir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Conv_Att_MCI_Dataset(img_type)\n",
    "dataset_trn, dataset_val, dataset_test = dataset.split_trn_val_test()\n",
    "dataloader_trn = DataLoader(dataset_trn, batch_size=batch_size, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BaseModel(img_type, backbone_freezing).to(device)\n",
    "# model = VGG16GradCAM(img_type, backbone_freezing).to(device)\n",
    "model = ConvAttnModel(img_type, h_dim_attn, n_heads, h_dim_fc, n_layers, backbone_freezing).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr, [beta_1, beta_2], eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'losses':{'trn':[], 'val':[], 'test':[]},\n",
    "    'corrects':{'trn':[], 'val':[], 'test':[]},\n",
    "    'accs':{'trn':[], 'val':[], 'test':[]},\n",
    "}\n",
    "es_queue, es_flag = deque(maxlen=es_size), False\n",
    "for epoch in range(n_epochs):\n",
    "    _losses, _corrects, n_tot = [],[],0\n",
    "    model.train()\n",
    "    for x, y, info in dataloader_trn:\n",
    "        for i in range(len(x)):\n",
    "            x[i] = x[i].to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        y_pred = model(x)\n",
    "        y_prob = y_pred.softmax(-1)[:,1]\n",
    "        loss = F.binary_cross_entropy(y_prob, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _correct = ((y_prob.cpu() >= 0.5) == info['labels']).sum()\n",
    "\n",
    "        _losses.append(loss.item())\n",
    "        _corrects.append(_correct)\n",
    "        n_tot += len(x[-1])\n",
    "    \n",
    "    accs = sum(_corrects) / n_tot\n",
    "    results['losses']['trn'].append(np.mean(_losses))\n",
    "    results['corrects']['trn'].append(sum(_corrects))\n",
    "    results['accs']['trn'].append(accs)\n",
    "\n",
    "    _losses, _corrects, n_tot = [],[],0\n",
    "    model.eval()\n",
    "    for x, y, info in dataloader_val:\n",
    "        for i in range(len(x)):\n",
    "            x[i] = x[i].to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        y_pred = model(x)\n",
    "        y_prob = y_pred.softmax(-1)[:,1]\n",
    "        loss = F.binary_cross_entropy(y_prob, y)\n",
    "\n",
    "        _correct = ((y_prob.cpu() >= 0.5) == info['labels']).sum()\n",
    "\n",
    "        _losses.append(loss.item())\n",
    "        _corrects.append(_correct)\n",
    "        n_tot += len(x[-1])\n",
    "    \n",
    "    accs = sum(_corrects) / n_tot\n",
    "    results['losses']['val'].append(np.mean(_losses))\n",
    "    results['corrects']['val'].append(sum(_corrects))\n",
    "    results['accs']['val'].append(accs)\n",
    "\n",
    "    _losses, _corrects, n_tot = [],[],0\n",
    "    model.eval()\n",
    "    for x, y, info in dataloader_test:\n",
    "        for i in range(len(x)):\n",
    "            x[i] = x[i].to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        y_pred = model(x)\n",
    "        y_prob = y_pred.softmax(-1)[:,1]\n",
    "        loss = F.binary_cross_entropy(y_prob, y)\n",
    "\n",
    "        _correct = ((y_prob.cpu() >= 0.5) == info['labels']).sum()\n",
    "\n",
    "        _losses.append(loss.item())\n",
    "        _corrects.append(_correct)\n",
    "        n_tot += len(x[-1])\n",
    "    \n",
    "    accs = sum(_corrects) / n_tot\n",
    "    results['losses']['test'].append(np.mean(_losses))\n",
    "    results['corrects']['test'].append(sum(_corrects))\n",
    "    results['accs']['test'].append(accs)  \n",
    "\n",
    "    es_queue.append(results['losses']['val'][-1])\n",
    "    if len(es_queue) >= es_size:\n",
    "        if (np.diff(es_queue) >= 0).all() and (np.diff(results['losses']['trn'][-es_size:]) < 0).all():\n",
    "            es_flag = True\n",
    "\n",
    "    if best_loss >= results['losses']['val'][-1]:\n",
    "        best_loss = results['losses']['val'][-1]\n",
    "        best_epoch = epoch\n",
    "        best_flag = True\n",
    "        \n",
    "        savepath = os.path.join(savedir, 'model_best.ckpt')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': best_loss,\n",
    "        }, savepath)\n",
    "\n",
    "    print(\"| Epoch %d/%d |\"%(epoch+1, n_epochs), end=' Early stopping!\\n' if es_flag else '\\n')\n",
    "    print(\"| Train      | Loss %6.2f | Acc. %6.2f |\"%(results['losses']['trn'][-1], results['accs']['trn'][-1]))\n",
    "    print(\"| Validation | Loss %6.2f | Acc. %6.2f |\"%(results['losses']['val'][-1], results['accs']['val'][-1]), end=' Best\\n' if best_flag else '\\n')\n",
    "    print(\"| Test       | Loss %6.2f | Acc. %6.2f |\"%(results['losses']['test'][-1], results['accs']['test'][-1]))\n",
    "    best_flag = False\n",
    "    savepath = os.path.join(savedir, 'model.ckpt')\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': best_loss,\n",
    "    }, savepath)\n",
    "    \n",
    "    if es_flag:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[10,5])\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.plot(results['losses']['trn'], label='Trn.')\n",
    "ax.plot(results['losses']['val'], label='Val.')\n",
    "ax.plot(results['losses']['test'], label='Test')\n",
    "ax.legend()\n",
    "ax.set_title('Loss')\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "ax.plot(results['accs']['trn'], label='Trn.')\n",
    "ax.plot(results['accs']['val'], label='Val.')\n",
    "ax.plot(results['accs']['test'], label='Test')\n",
    "ax.legend()\n",
    "ax.set_title('Accuracy')\n",
    "\n",
    "savepath_lc = os.path.join(savedir, 'learning_curve.png')\n",
    "plt.savefig(savepath_lc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadpath = os.path.join(savedir, 'model_best.ckpt')\n",
    "ckpt = torch.load(loadpath)\n",
    "\n",
    "model.load_state_dict(ckpt['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_attention(m):\n",
    "    forward_orig = m.forward\n",
    "\n",
    "    def wrap(*args, **kwargs):\n",
    "        kwargs[\"need_weights\"] = True\n",
    "        kwargs[\"average_attn_weights\"] = False\n",
    "\n",
    "        return forward_orig(*args, **kwargs)\n",
    "\n",
    "    m.forward = wrap\n",
    "attn_layers = []\n",
    "for i in range(len(dataset.img_type)):\n",
    "    attn_layers.append(model.attns[i].layers[-1].self_attn)\n",
    "    patch_attention(attn_layers[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_features, attn_inputs, attn_outputs = {_type:[] for _type in dataset.img_type}, {_type:[] for _type in dataset.img_type}, {_type:[] for _type in dataset.img_type}\n",
    "hooks = []\n",
    "for i, _type in enumerate(['copy', 'trail', 'clock']): # dataset.img_type\n",
    "  hooks.append(model.vgg16_models[i][-1].register_forward_hook(\n",
    "      lambda self, input, output, _type=_type: vgg16_features[_type].append(output)\n",
    "    ))\n",
    "  hooks.append(attn_layers[i].register_forward_hook(\n",
    "      lambda self, input, output, _type=_type: attn_inputs[_type].append(input[0])\n",
    "    ))\n",
    "  hooks.append(attn_layers[i].register_forward_hook(\n",
    "      lambda self, input, output, _type=_type: attn_outputs[_type].append(output)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 130\n",
    "imgs = []\n",
    "for _type in dataset.img_type:\n",
    "    imgs.append(dataset_test.dataset[_type][idx:idx+1].to(device))\n",
    "score = dataset_test.dataset['scores'][idx:idx+1]\n",
    "label = dataset_test.dataset['labels'][idx:idx+1]\n",
    "print(imgs[-1].shape, score, label)\n",
    "\n",
    "y_pred = model(imgs)\n",
    "# Grad-CAM\n",
    "# class_idx = y_pred.argmax(-1).item()\n",
    "# heatmap = model.generate_cam(img, class_idx).cpu()\n",
    "# print(heatmap.shape)\n",
    "\n",
    "# Self-attention\n",
    "for hook in hooks:\n",
    "    hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "heatmaps = []\n",
    "for _type in dataset.img_type:\n",
    "    B,C,H,W = vgg16_features[_type][0].shape\n",
    "\n",
    "    attn_weights = attn_outputs[_type][0][1]\n",
    "    heatmap = attn_weights[:,0,0,1:].reshape(B,H,W).detach().cpu()\n",
    "    heatmap = torch.clamp(heatmap, min=0)\n",
    "    heatmap /= heatmap.max()\n",
    "\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = np.uint8(Image.fromarray(heatmap[0]).resize((imgs[-1].shape[2], imgs[-1].shape[3])))\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    heatmaps.append(heatmap)\n",
    "\n",
    "# alpha = 1.0\n",
    "# superimposed_img = heatmap * alpha #+ img[0].permute(1,2,0).detach().cpu().numpy()\n",
    "# superimposed_img = np.clip(superimposed_img, 0, 255).astype(np.uint8)\n",
    "\n",
    "fig = plt.figure(figsize=(5*len(dataset.img_type), 5))\n",
    "fig.suptitle('Pred: %d vs. GT: %d'%(y_pred.argmax(-1).item(), label.item()))\n",
    "for i, _type in enumerate(dataset.img_type):\n",
    "    ax = fig.add_subplot(1, len(dataset.img_type), i+1)\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "\n",
    "    im = ax.imshow(imgs[i][0].detach().cpu().permute(1,2,0))   \n",
    "    ax.set_title(_type)\n",
    "    im = ax.imshow(heatmaps[i], alpha=0.5)\n",
    "    fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "    ax.axis('off')\n",
    "savepath_fig_res = os.path.join(savedir, 'results.png')\n",
    "plt.savefig(savepath_fig_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
